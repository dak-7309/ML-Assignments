# -*- coding: utf-8 -*-
"""MLq1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13XdIIW31bsr-pTMXf2hwLwcq2BJcZ4Cg
"""

# from google.colab import drive
# drive.mount('/content/drive/')

from sklearn.utils import shuffle
from math import *
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import check_random_state
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import pickle
from sklearn.decomposition import TruncatedSVD
from sklearn.manifold import TSNE
import seaborn as sns
from sklearn.neural_network import MLPClassifier

class MyNeuralNetwork():
    
    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax']
    weight_inits = ['zero', 'random', 'normal']

    def __init__(self, n_layers, layers_sizes, activation='sigmoid', learning_rate=0.1, weight_init='normal', batch_size=1000, num_epochs=100):
        
        """
        Initializing a new MyNeuralNetwork object

        Parameters
        ----------
        n_layers : int value specifying the number of layers

        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer

        activation : string specifying the activation function to be used
                     possible inputs: relu, sigmoid, linear, tanh

        learning_rate : float value specifying the learning rate to be used

        weight_init : string specifying the weight initialization function to be used
                      possible inputs: zero, random, normal

        batch_size : int value specifying the batch size to be used

        num_epochs : int value specifying the number of epochs to be used
        """
        self.layers_sizes=layers_sizes[1:]
        self.activation=activation
        self.learning_rate=learning_rate
        self.weight_init=weight_init
        self.batch_size=batch_size
        self.num_epochs=num_epochs
        self.weights={}
        self.n_layers=len(self.layers_sizes)
        self.num_samples=0
        self.training_loss_values=[]
        self.testing_loss_values=[]
        self.gg=0.01
        self.XTEST=None
        self.YTEST=None
        self.TTTT=None

        if activation not in self.acti_fns:
            raise Exception('Incorrect Activation Function')

        if weight_init not in self.weight_inits:
            raise Exception('Incorrect Weight Initialization Function')
        pass


    def relu(self, X):
        """
        Calculating the ReLU activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        return np.maximum(0,X)

    def relu_grad(self, X):
        """
        Calculating the gradient of ReLU activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        X[X<=0]=0
        X[X>0]=1
        return X

    def sigmoid(self, X):
        """
        Calculating the Sigmoid activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """

        return 1.0/(1.0+np.exp(-X))

    def sigmoid_grad(self, X):
        """
        Calculating the gradient of Sigmoid activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        var=self.sigmoid(X)
        return var*(1-var)

    def linear(self, X):
        """
        Calculating the Linear activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        return X

    def linear_grad(self, X):
        """
        Calculating the gradient of Linear activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        X[X<=0]=1
        X[X>0]=1
        return X

    def tanh(self, X):
        """
        Calculating the Tanh activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))
      
    def tanh_grad(self, X):
        """
        Calculating the gradient of Tanh activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        return 1-self.tanh(X)**2

    def softmax(self, Z):
        """
        Calculating the ReLU activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        expZ=np.exp(Z-np.max(Z))
        return expZ/expZ.sum(axis=0,keepdims=True)

    def softmax_grad(self, X):
        """
        Calculating the gradient of Softmax activation for a particular layer

        Parameters
        ----------
        X : 1-dimentional numpy array 

        Returns
        -------
        x_calc : 1-dimensional numpy array after calculating the necessary function over X
        """
        s = X.reshape(-1,1)
        return np.diagflat(X)-np.dot(s,np.traspose(s))

    def zero_init(self, shape):
        """
        Calculating the initial weights after Zero Activation for a particular layer

        Parameters
        ----------
        shape : tuple specifying the shape of the layer for which weights have to be generated 

        Returns
        -------
        weight : 2-dimensional numpy array which contains the initial weights for the requested layer
        """
        return np.zeros((shape[0],shape[1]))

    def random_init(self, shape):
        """
        Calculating the initial weights after Random Activation for a particular layer

        Parameters
        ----------
        shape : tuple specifying the shape of the layer for which weights have to be generated 

        Returns
        -------
        weight : 2-dimensional numpy array which contains the initial weights for the requested layer
        """
        return np.random.randn(shape[0],shape[1])*0.01

    def normal_init(self, shape):
        """
        Calculating the initial weights after Normal(0,1) Activation for a particular layer

        Parameters
        ----------
        shape : tuple specifying the shape of the layer for which weights have to be generated 

        Returns
        -------
        weight : 2-dimensional numpy array which contains the initial weights for the requested layer
        """
        return np.random.normal(size=(shape[0],shape[1]))*0.01

    def initialize_parameters(self):
        #random seed value for consistent results
        np.random.seed(20)

        # using a dictionary of weights as a class object attribute with keys weight and bias
        # initialises weights based on weight_init value, of size (current layer size)x(precious layer size)
        for i in range(1,len(self.layers_sizes)):
            if self.weight_init=="random":
              self.weights["weight"+str(i)]=self.random_init((self.layers_sizes[i],self.layers_sizes[i-1]))
              self.weights["bias"+str(i)]=self.zero_init((self.layers_sizes[i],1))

            if self.weight_init=="normal":
              self.weights["weight"+str(i)]=self.normal_init((self.layers_sizes[i],self.layers_sizes[i-1]))
              self.weights["bias"+str(i)]=self.zero_init((self.layers_sizes[i],1))

            if self.weight_init=="zero":
              self.weights["weight"+str(i)]=self.zero_init((self.layers_sizes[i],self.layers_sizes[i-1]))
              self.weights["bias"+str(i)]=self.zero_init((self.layers_sizes[i],1))


    def forward(self,X):
      # forward phase for the neural network
      output=np.transpose(X)
      d_collection={}
      # this dict will store input, weights and outputs of activation at each neuron in each layer 

      for l in range(self.n_layers-1):
          # wx+b form
          input=self.weights["weight"+str(l+1)].dot(output)+self.weights["bias"+str(l+1)]
          if self.activation=="sigmoid":
            output=self.sigmoid(input)
          if self.activation=="linear":
            output=self.linear(input)
          if self.activation=="relu":
            output=self.relu(input)
          if self.activation=="softmax":
            output=self.softmax(input)
          if self.activation=="tanh":
            output=self.tanh(input)
          # applying activation fn based on activation value

          d_collection["input"+str(l+1)]=input
          d_collection["weight"+str(l+1)]=self.weights["weight"+str(l+1)]
          d_collection["output"+str(l+1)]=output
      self.TTTT=output

      # output of layer becomes the input of next layer and process repeats iteratively
      input=self.weights["weight"+str(self.n_layers)].dot(output)
      input+=self.weights["bias"+str(self.n_layers)]
      output=self.softmax(input)
      # applying softmax at final layer to yield probabilities

      d_collection["input"+str(self.n_layers)]=input
      d_collection["output"+str(self.n_layers)]=output
      d_collection["weight"+str(self.n_layers)]=self.weights["weight"+str(self.n_layers)]

      return output,d_collection



    def backward(self, X, Y, d_collection):
        # backpropagation of neural network
        derivatives={}
        d_collection["output0"]=np.transpose(X)
        output=d_collection["output"+str(self.n_layers)]

        dinput=output-np.transpose(Y)
        # derivative term at the final layer, where both real_y and predicted_y are known

        dbias=np.sum(dinput,axis=1,keepdims=True)/self.batch_size
        value=np.transpose(d_collection["output"+str(self.n_layers-1)])
        dweight=dinput.dot(value)
        dweight/=self.batch_size
        # determing change in weights for cross entropy loss after obtaining formulas from mathematical analysis
        # and chain rule
        
        weighted_input=np.transpose(d_collection["weight"+str(self.n_layers)])
        weighted_input=weighted_input.dot(dinput)
        derivatives["dweight"+str(self.n_layers)]=dweight
        derivatives["dbias"+str(self.n_layers)]=dbias
 
        for i in range(self.n_layers-1,0,-1):
            # applying gradient of activation fn on the basis of activation value
            if self.activation=="sigmoid":
              grad=self.sigmoid_grad(d_collection["input"+str(i)])
            if self.activation=="relu":
              grad=self.relu_grad(d_collection["input"+str(i)])
            if self.activation=="linear":
              grad=self.linear_grad(d_collection["input"+str(i)])
            if self.activation=="tanh":
              grad=self.tanh_grad(d_collection["input"+str(i)])
            if self.activation=="softmax":
              grad=self.softmax_grad(d_collection["input"+str(i)])

            dinput=weighted_input*grad
            dbias=np.sum(dinput,axis=1,keepdims=True)/self.batch_size
            value=np.transpose(d_collection["output"+str(i-1)])
            dweight=dinput.dot(value)
            dweight/=self.batch_size
            # iteratively finds change in weights, i.e. the derivative terms and
            # stores them in the dict below
            if i>=2:
                weighted_input=np.transpose(d_collection["weight"+str(i)])
                weighted_input=weighted_input.dot(dinput)

            derivatives["dbias"+str(i)]=dbias
            derivatives["dweight"+str(i)]=dweight

        return derivatives

    def weight_update(self, derivatives):
      # weight update function is called in fit, in every opoch, to update the weights after a forward
      # and backprop traversal and optimising weights
      for j in range(1,self.n_layers+1):
        self.weights["bias"+str(j)]-=self.learning_rate*derivatives["dbias"+str(j)]
        self.weights["weight"+str(j)]-=self.learning_rate*derivatives["dweight"+str(j)]

    def fit(self, X, Y):
        """
        Fitting (training) the linear model.

        Parameters
        ----------
        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.

        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.
        
        Returns
        -------
        self : an instance of self
        """
        np.random.seed(40)
        self.num_samples=X.shape[0]
        self.layers_sizes.insert(0,X.shape[1])
        self.initialize_parameters()
        variable=self.num_epochs//5

        # loop for epochs
        for vv in range(self.num_epochs):
            # creating batches of dataset of specified batch size
            X,Y=shuffle(X,Y,random_state=vv)
            num_batches=X.shape[0]//self.batch_size
            train_x=np.vsplit(X,num_batches)
            train_y=np.vsplit(Y,num_batches)
            train_cost=0
            
            for i in range(num_batches):
              # iterating over batches and applying forward and backward propagation
              # and determining training cost (cross entropy loss) for every batch
              # and averaging them to give a generalised loss
              A,d_collection=self.forward(train_x[i])
              train_cost+=(-np.mean(train_y[i]*np.log(np.transpose(A))))/num_batches
              derivatives=self.backward(train_x[i],train_y[i],d_collection)

              self.weight_update(derivatives)
              
            if vv%variable==0:
                print("Accuracy score:",self.score(X,Y))
            
            # adding both training and testing losses in a list to plot in further ques
            self.training_loss_values.append(train_cost)
            test_cost=-np.mean(self.YTEST*np.log(np.transpose(self.predict_proba(self.XTEST))))
            self.testing_loss_values.append(test_cost)
        return self

    def predict_proba(self, X):
        """
        Predicting probabilities using the trained linear model.

        Parameters
        ----------
        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.

        Returns
        -------
        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the 
            class wise prediction probabilities.
        """
        # yields probabilities at last layer after forward propagation
        A,cache=self.forward(X)
        return A

    def predict(self, X):
        """
        Predicting values using the trained linear model.

        Parameters
        ----------
        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.

        Returns
        -------
        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.
        """
        # yields labels of the given dataset X after calling predict_proba
        A=self.predict_proba(X)
        y_hat=np.argmax(A,axis=0)
        return y_hat
    
    def score(self, X, Y):
        """
        Predicting values using the trained linear model.

        Parameters
        ----------
        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.

        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.

        Returns
        -------
        acc : float value specifying the accuracy of the model on the provided testing set
        """
        # calls predict on X and predicts labels, and compares it with true labels to return accuracy
        y_hat=self.predict(X)
        Y=np.argmax(Y,axis=1)
        atrain_costuracy=(y_hat==Y).mean()
        return atrain_costuracy

    def plot_cost(self):
        # plots training loss vs epoch, testing loss vs epoch
        plt.figure()
        plt.plot(np.arange(len(self.training_loss_values)),self.training_loss_values, 'b-', label='Training Loss vs epoch')
        plt.plot(np.arange(len(self.testing_loss_values)),self.testing_loss_values, 'r-', label='Testing Loss vs epoch')
        plt.xlabel("Epochs")
        plt.ylabel("Loss Values")
        plt.legend()
        plt.show()

