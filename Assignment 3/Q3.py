# -*- coding: utf-8 -*-
"""MLq3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S022igfdMfIKbWwCCuwgM8bR537GzXZt
"""

# from google.colab import drive
# drive.mount('/content/drive/')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
# importing all libraries required for pytorch

# reading my training and validation datasets into pandas dataframes
df_train=pd.read_csv('largeTrain.csv',header=None)
df_valid=pd.read_csv('largeValidation.csv',header=None)

# defined a custom class that has the desired methods __init,__getitem__ abd __len__ needed for the DataLoader Class
class MyClass(Dataset):
  def __init__(self,data):
    self.data=data
    self.X=data.iloc[:,1:]
    self.Y=data.iloc[:,0]
    # return X and Y from dataset, Y is the 1st column
    
    # feature size=number of columns=129-1=128
    self.feature_size=len(self.X.columns)
    # label_size=10 here as there are 10 unique values in Y
    self.label_size=len(pd.unique(self.Y))
  
  def __getitem__(self,ind):
    # returns data and its label (by index or row provided)
    X=torch.tensor(self.X.iloc[ind].values.astype(float))
    Y=torch.tensor(int(self.Y.iloc[ind]))
    return (X,Y)
  def __len__(self):
    # returns length of dataset
    return (len(self.data))



# my custom neural network class that contains information about my network structure
# and has a forward pass method that applies activation of my data
class MyNeuralNetwork(nn.Module):
  def __init__(self, inp_size, out_size, hidden_units=4):
      super().__init__()
      
      # input and output layer
      self.input_fc=nn.Linear(inp_size, hidden_units)
      self.output_fc=nn.Linear(hidden_units, out_size)
        
  def forward(self, X):
      batch_size=X.shape[0]
      X=X.view(batch_size,-1)
      # relu activation after input
      h=F.relu(self.input_fc(X))
      y_pred=self.output_fc(h)
      # returns data post activation and labels
      return y_pred,h

# train is used to train my model for the provided iterator and performs forward and backward
# prop and takes into consideration the inputted optimizer, criterion and device
def train(model, iterator, optimizer, criterion, device):

    epoch_loss=0
    model.train()
    
    for (x,y) in iterator:
        x=x.to(device)
        y=y.to(device)
        optimizer.zero_grad()
        # will apply my neural network model on x
        y_pred,_=model(x.float())
        loss=criterion(y_pred,y)
        loss.backward()
        optimizer.step()
        # adds loss to this variable after backward prop
        epoch_loss+=loss.item()
    # return average loss per batch
    return epoch_loss/len(iterator)

# evaluate is used for testing data for the provided iterator and performs forward 
# prop and takes into consideration the inputted optimizer, criterion and device
def evaluate(model, iterator, criterion, device):
    
    epoch_loss=0
    model.eval()
    
    with torch.no_grad():
        for (x,y) in iterator:
            x=x.to(device)
            y=y.to(device)
            # applies my neural net model to x
            y_pred,_=model(x.float())
            loss=criterion(y_pred,y)
            # adds loss to variable
            epoch_loss+=loss.item()
    # returns loss per batch
    return epoch_loss/len(iterator)

def func_find_losses(train_iterator, valid_iterator, learning_rate, hidden_units, epochs, feature_size, label_size):
  # function that initialises my model based on the iterators created and features',hidden units' and labels' sizes
  model=MyNeuralNetwork(feature_size,label_size,hidden_units=hidden_units)

  # cross entropy loss is the criterion as asked
  criterion=nn.CrossEntropyLoss()
  # adam model's parameters with custom learning rate as provided in argument
  optimizer=optim.Adam(model.parameters(),lr=learning_rate)
  # device is either pu or gpu based on availability
  device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model=model.to(device)
  criterion=criterion.to(device)

  train_losses,val_losses=epoch_loss(epochs,model,train_iterator,valid_iterator,optimizer,criterion,device)

  del model
  return train_losses, val_losses

def epoch_loss(epochs,model,train_iterator,valid_iterator,optimizer,criterion,device):
  
  train_losses=[]
  val_losses=[]
  # initialised a list that will contain losses for all epochs
  # loop over epoch
  for epoch in range(epochs):
    # train loss obtained by model training
    train_loss=train(model,train_iterator,optimizer,criterion,device)
    # test loss obtained by model evaluating
    val_loss=evaluate(model,valid_iterator,criterion,device)
    train_losses.append(train_loss)
    val_losses.append(val_loss)

  return train_losses,val_losses

EPOCHS = 100
BATCH_SIZE=256
# custom values

# creates training and validation objects of MyClass needed for DataLoader
train_data=MyClass(df_train)
valid_data=MyClass(df_valid)

feature_size = train_data.feature_size
label_size =train_data.label_size

# DataLoader objects with custom batch_size yields iterators for my model
train_iterator=DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)
valid_iterator=DataLoader(valid_data,batch_size=BATCH_SIZE,shuffle=True)

# For number of hidden objects
hidden_units=[5,20,50,100,200]

# individually finding training,validation losses for the combination of hidden units
train_loss_1,val_loss_1=func_find_losses(train_iterator,valid_iterator,0.01,5,EPOCHS,feature_size,label_size)
train_loss_2,val_loss_2=func_find_losses(train_iterator,valid_iterator,0.01,20,EPOCHS,feature_size,label_size)
train_loss_3,val_loss_3=func_find_losses(train_iterator,valid_iterator,0.01,50,EPOCHS,feature_size,label_size)
train_loss_4,val_loss_4=func_find_losses(train_iterator,valid_iterator,0.01,100,EPOCHS,feature_size,label_size)
train_loss_5,val_loss_5=func_find_losses(train_iterator,valid_iterator,0.01,200,EPOCHS,feature_size,label_size)

# stroring in a list and plotting against number of hidden units
train_losses=[train_loss_1[-1],train_loss_2[-1],train_loss_3[-1],train_loss_4[-1],train_loss_5[-1]]
val_losses=[val_loss_1[-1],val_loss_2[-1],val_loss_3[-1],val_loss_4[-1],val_loss_5[-1]]

plt.plot(hidden_units,train_losses,'r',label='Training Loss')
plt.plot(hidden_units,val_losses,'b',label='Validation Loss')

plt.xlabel("Hidden Units")
plt.ylabel("Loss")
plt.title("Learning Rate= "+str(0.01)+" Hidden Units= "+str(hidden_units))
plt.legend()
plt.show()
# plots



# For Learning Rate
learning_rates = [0.1, 0.01, 0.001]
epochs_collection=[]

for i in range(EPOCHS):
    epochs_collection.append(i)

# looping over epoch and possible combinations of learning rate and plots the losses obtained against epoch
for i in range(3):
  train_loss, val_loss = func_find_losses(train_iterator, valid_iterator, learning_rates[i], 4, EPOCHS, feature_size, label_size)
  plt.plot(epochs_collection,train_loss,'r',label='Training Loss')
  plt.plot(epochs_collection,val_loss,'b',label='Validation Loss')
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.title("Learning Rate= "+str(learning_rates[i])+" Hidden Units= 4")
  plt.legend()
  plt.show()